# selfhosted-Codestral
run Codestral 22b LLM locally using llama-server (llama.cpp) and expose it publicly through a reverse SSH tunnel on your VPS
